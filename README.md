CBAM Analysis and Prediction
Project Overview
This project is an end-to-end machine learning pipeline that uses a synthetic dataset to predict a product's status under the EU's Carbon Border Adjustment Mechanism (CBAM). The goal is to demonstrate a machine learning workflow for a classification problem with a real-world application in logistics and compliance.

The dataset, generated by ChatGPT, simulates customs data for a company's imports. The model provides a simplified prediction and calculates a potential CBAM levy based on a set of assumptions.

Features Used
The model uses the following features from the dataset to make its predictions:

HS_Code: The Harmonized System code for the imported goods.

Country_of_Origin: The country where the goods originated.

Quantity_tonnes: The total weight of the goods in tonnes.

Value_EUR: The declared value of the goods in Euros.

CO2_Embedded_tCO2: The estimated embedded CO2 per tonne.

Transport_Mode: The method of transport (e.g., Sea, Air, Road).

Customs_Regime: The customs procedure applied to the goods.

Sector: The industry sector of the goods (e.g., Fertiliser, Steel).

Methodology
The project follows a standard machine learning workflow:

Data Preprocessing: The dataset was cleaned and prepared for the model. Categorical features were encoded using LabelEncoder, and numerical features were scaled using StandardScaler.

Model Building: A tensorflow.keras.Sequential neural network was built for binary classification. The model consists of multiple Dense layers with relu activation and a final sigmoid activation layer.

Training and Evaluation: The data was split into training and testing sets, and the model was trained. Performance was evaluated using a confusion matrix and accuracy metrics to understand its predictive capabilities.

Assumptions and Limitations
Simplified Model: This project uses a simplified set of features for demonstration purposes. A real-world CBAM prediction model would require a much more extensive set of data and features.

Synthetic Data: The dataset is synthetic and was generated by ChatGPT.

CBAM Levy Calculation: The calculation for the potential CBAM levy is based on a fixed, assumed rate of 73.21 euros per tonne of CO2. This rate is for illustrative purposes only and does not reflect current or future market prices.

Key Findings
The model successfully demonstrates the ability to classify goods as either "Subject to CBAM" or "Not Subject to CBAM," providing a foundation for more advanced compliance and cost management tools.

How to Run the Code
Clone this repository to your local machine.

Install the required Python libraries using pip:

pip install pandas numpy scikit-learn tensorflow matplotlib seaborn

Open the CBAM_analysis_and_prediction.ipynb notebook in a Jupyter environment (e.g., JupyterLab or VS Code).

Run the cells in the notebook to see the full analysis and prediction pipeline in action.

Future Work
Hyperparameter Tuning: Explore different model architectures and hyperparameters to optimize performance.

Additional Features: Integrate more real-world features that may impact CBAM status.

Advanced Evaluation: Implement cross-validation for a more robust evaluation of the model's performance.

Author
Ricky Ansong

Acknowledgments
This project was inspired by the need for automated compliance tools in logistics.

The dataset was synthetically generated by ChatGPT.

